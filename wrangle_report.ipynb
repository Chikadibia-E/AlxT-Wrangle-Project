{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Data wrangling sometimes referred to as data munging, is the process of transforming and mapping data from one \"raw\" data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics - [Wikipedia](https://en.wikipedia.org/wiki/Data_wrangling).\n",
    "\n",
    "The dataset that was wrangled was the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comments about the dog. These ratings almost always have a denominator of 10. The numerators, almost always are greater than 10. for example; 11/10, 12/10, 13/10, etc. Because \"they're good dogs, Brent.\" WeRateDogs has over 4 million followers and has received international media coverage.\n",
    "\n",
    "\n",
    "The following datasets (for WeRateDogs) were used for the data wrangling:\n",
    "1. [twitter_archive_enhanced.csv](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv) - This file was downloaded manually by clicking the provided link and uploaded to jupyter notebook workspace for wrangling activities.\n",
    "\n",
    "2. [image_predictions.tsv]( https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv) - This file was programmatically downloaded from the given URL via the use of the python request library.\n",
    "\n",
    "3. tweet_json.txt - This file was gathered using the tweet IDs in the WeRateDogs Twitter archive, querying the Twitter API for each tweet's JSON data using Python's Tweepy library and stored each tweet's entire set of JSON data in a file called tweet_json.txt file. Then this .txt file was read line by line into a pandas DataFrame with tweet ID, retweet count, and favorite count.\n",
    "\n",
    "The data wrangling was done in 3 steps - Gathering data, Assessing data, and Cleaning data;\n",
    "\n",
    "### Gathering data\n",
    "* twitter_archive_enhanced.csv was manually downloaded from the given link and uploaded for wrangling\n",
    "* image_predictions.tsv was downloaded programmatically using Python's request library\n",
    "* tweet_json.txt was gathered using Python's Tweepy library and Twitter API and then stored for wrangling\n",
    "\n",
    "### Assessing data\n",
    "\n",
    "The three data sets gathered were assessed both visually (using Microsoft Excel and pandas data frame) and programmatically using Python functions. The datasets were assessed for quality issues and tidiness issues. Issues discovered were cleaned at the cleaning stage.\n",
    "\n",
    "### Cleaning data\n",
    "\n",
    "At this stage, more than 8 issues found during the assessment stage were programmatically corrected using python functions. After this stage, the master dataset was ready for storage, exploration, and analysis.\n",
    "\n",
    "Note: The data cleaning done here was not exhaustive. Issues like the existence of tags and 'hrefs' in the contents of the source column and the existence of the rating fraction with the tweet text in the contents of the text column were not addressed. These and other issues to be found are recommended for further review and cleaning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
